<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Puru Ojha</title>

  <meta name="author" content="Puru Ojha">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Puru Ojha
                  </p>
                  <p>
                    I'm a Master's student at the International Institute of Information Technology, Hyderabad. My
                    research bridges the gap between semantic reasoning and physical reality in robotics. I focus on
                    developing hierarchical systems that enable Vision-Language Models to generate probabilistic spatial
                    plans while ensuring they remain grounded in the kinematic and dynamic constraints of real robot
                    embodiments. My work addresses the fundamental challenge of long-horizon manipulation: how to
                    combine the semantic common sense of foundation models with the geometric rigor required for
                    reliable physical interaction.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:puru.ojha@research.iiit.ac.in">Email</a> &nbsp;/&nbsp;
                    <a href="Resume_Puru_Ojha.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="Research Statement 4 new.pdf">Research Statement</a> &nbsp;/&nbsp;
                    <a href="SOP_UMich.pdf">SOP</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/puru-ojha-a27632166/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/mechapuru">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/Profile_Pix.jpeg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/Profile_Pix.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    Modern robotics is fractured between two paradigms: end-to-end Vision-Language-Action models that
                    possess semantic understanding but often hallucinate physics, and classical planning systems that
                    are geometrically rigorous but rely on brittle, hand-engineered abstractions. My research aims to
                    bridge this divide through <strong>Hierarchical Action Reasoning Models (HARMs)</strong>—a framework
                    where high-level semantic reasoning is grounded via <strong>Probabilistic Spatial Contracts</strong>
                    and verified through <strong>Embodiment-Aware Causal Verification</strong>. Rather than treating
                    VLMs as isolated commanders that output deterministic actions, I develop systems where spatial
                    intent is represented as a distribution of feasible interactions, enabling robots to sample
                    trajectories that are both semantically logical and physically executable. This approach combines
                    the generalization of foundation models with the reliability required for real-world deployment.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Selected Projects</h2>
                  <p style="margin-top: 8px; color: #666;">Flagship research projects demonstrating my approach to
                    bridging semantic reasoning and physical reality in robotic systems.</p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr bgcolor="#ffffd0">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <span class="papertitle"><strong>DCROSS: Diffusion-Enabled Cross-Embodiment Policy
                      Transfer</strong></span>
                  <br>
                  <em>Ongoing Research</em> | Paper (In Preparation) | Code | Video
                  <p style="margin-top: 12px; margin-bottom: 8px;">
                    <strong>Core Contribution:</strong> Developed a zero-shot policy transfer framework enabling
                    policies trained on Franka Panda to control morphologically distinct robots (xArm) without
                    fine-tuning, addressing the fundamental challenge of embodiment mismatch in foundation models.
                  </p>
                  <p style="margin-bottom: 8px;">
                    <strong>Technical Approach:</strong> Designed a hybrid pipeline combining (1) a Brownian bridge
                    diffusion model for visual domain adaptation, translating live observations into the source robot's
                    visual domain, and (2) kinematic retargeting for action space mapping. Through extensive real-world
                    experimentation, identified that pixel-space adaptation is insufficient—leading to insights about
                    object-centric 3D representations for generalizable autonomy.
                  </p>
                  <p style="margin-bottom: 0;">
                    <strong>Impact:</strong> This work directly informs my PhD research agenda on Probabilistic
                    Object-Centric World Models, demonstrating the necessity of disentangling task dynamics from robot
                    morphology.
                  </p>
                </td>
              </tr>

              <tr bgcolor="#ffffd0">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <span class="papertitle"><strong>PHELPS: Planning Hierarchical Execution for Long-Horizon
                      Possibilities</strong></span>
                  <br>
                  <em>Ongoing Research</em> | Paper (In Preparation) | Code | Video
                  <p style="margin-top: 12px; margin-bottom: 8px;">
                    <strong>Core Contribution:</strong> Led development of a hierarchical long-horizon manipulation
                    system where VLMs generate symbolic contracts for low-level policy execution, revealing critical
                    gaps in how semantic planners interface with physical controllers.
                  </p>
                  <p style="margin-bottom: 8px;">
                    <strong>Technical Approach:</strong> Integrated VLM-based high-level planning with PDDL-inspired
                    contract representation, modified RLBench for composite task evaluation, and implemented motion
                    planning controllers. System successfully decomposed complex tasks but revealed failure modes when
                    VLMs hallucinated affordances or issued kinematically infeasible commands.
                  </p>
                  <p style="margin-bottom: 0;">
                    <strong>Impact:</strong> Failures were more instructive than successes—crystallized my core research
                    insight that we need <strong>Embodiment-Aware Causal Verification</strong> to ground VLM plans in
                    learned feasibility models, not just symbolic predicates. This directly motivates my proposed Latent
                    Causal Verifier framework.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;border-bottom:1px solid #eee;">
                  <a>
                    <span class="papertitle">ACMGVR: Architecturally Consistent Mazes for Games in Virtual
                      Reality</span>
                  </a>
                  <br>
                  <strong>Puru Ojha</strong>, Y. Raghu Reddy
                  <br>
                  <em>CHI PLAY</em>, 2024 &nbsp;•&nbsp;
                  <a href="https://doi.org/10.1145/3665463.3678818">Paper</a> | Code | Video | Poster
                  <p style="margin-top: 8px; margin-bottom: 0;"></p>
                  <p>
                    Engineered a framework for procedural generation of complex, structured environments for
                    benchmarking RL agents and studying sim-to-real transfer of navigational policies.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;">
                  <a>
                    <span class="papertitle">The POMS Effect: Measuring the Impact of Overlapping Architectures on User
                      Engagement in Virtual Reality</span>
                  </a>
                  <br>
                  <strong>Puru Ojha</strong>, Aaditya Narain, Y. Raghu Reddy
                  <br>
                  <em>ICVR</em>, 2025 &nbsp;•&nbsp;
                  <a href="Research Papers/The POMS Effect.pdf">Paper</a> | Code | Video | Poster
                  <p style="margin-top: 8px; margin-bottom: 0;"></p>
                  <p>
                    Investigated how overlapping architectural structures in virtual reality environments affect user
                    navigation and engagement, providing insights for designing more intuitive VR spaces.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Additional Projects</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;border-bottom:1px solid #eee;">
                  <span class="papertitle">Sequential Rearrangement Planning with Language-Guided Graph
                    Transformer</span>
                  <br>
                  Paper | Code | Video | Poster
                  <p style="margin-top: 8px; margin-bottom: 0;">Designed a graph-based transformer model for predicting
                    object removal sequences in cluttered
                    tabletop scenes, conditioned on natural language goals. Built a large-scale simulation and training
                    pipeline with expert A* supervision and PPO fine-tuning, achieving state-of-the-art performance over
                    GRN and NRP baselines.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;">
                  <span class="papertitle">Enhanced Transformer-Based Framework for Grounded Image Situation
                    Recognition</span>
                  <br>
                  <a href="Research Papers/NeurIPS_2023.pdf">Paper</a> | Code | Video | Poster
                  <p style="margin-top: 8px; margin-bottom: 0;">Developed a transformer framework for grounded situation
                    recognition to enable robots to follow
                    complex natural language instructions. Improved noun grounding and verb prediction by integrating
                    CLIP and Faster-RCNN features into a CoFormer architecture, achieving state-of-the-art performance
                    on the SWiG dataset.</p>
                </td>
              </tr>

            </tbody>
          </table>


          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
            <tbody>
              <tr>
                <td>
                  <h2>Teaching & Mentorship</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:16px;width:25%;vertical-align:top">
                  <strong style="font-size: 15px;">AI Olympiad Coach</strong>
                  <br>
                  <em>IIIT Hyderabad</em>
                  <br>
                  <span style="color: #666; font-size: 13px;">2024-2025</span>
                </td>
                <td style="padding:8px;width:75%;vertical-align:middle">
                  Coached India's IOAI 2025 team, teaching reinforcement learning and motion planning. Bridged classical
                  robotics concepts with modern deep RL techniques (DQN, PPO), preparing students for international
                  competition in embodied AI.
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:25%;vertical-align:top">
                  <strong style="font-size: 15px;">Teaching Assistant</strong>
                  <br>
                  <em>Mobile Robotics, IIIT-H</em>
                  <br>
                  <span style="color: #666; font-size: 13px;">2023-2024</span>
                </td>
                <td style="padding:8px;width:75%;vertical-align:middle">
                  Supported instruction and student projects on robot kinematics, SLAM, and path planning using ROS and
                  Gazebo. Mentored students in implementing motion planning algorithms and debugging real-world robotic
                  systems.
                </td>
              </tr>

            </tbody>
          </table>

          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
            <tbody>
              <tr>
                <td>
                  <h2>Technical Skills</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <p style="margin: 0;">
                    <strong>Languages & Tools:</strong> Python, C++, C#, Bash, Git, Linux, Tmux
                  </p>
                  <p style="margin: 8px 0 0 0;">
                    <strong>Robotics & Simulation:</strong> ROS, PyBullet, Gazebo, MoveIt, RViz, OpenCV, PCL, Unity,
                    Blender
                  </p>
                  <p style="margin: 8px 0 0 0;">
                    <strong>Deep Learning & RL:</strong> PyTorch, Stable Baselines3, TensorFlow, CLIP, PointNet++,
                    PyTorch Geometric, RLlib, HDF5
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's
                      website</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>