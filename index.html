<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Puru Ojha</title>

  <meta name="author" content="Puru Ojha">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Puru Ojha
                  </p>
                  <p>
                    I'm a Master's student at the International Institute of Information Technology, Hyderabad. My
                    research bridges the gap between semantic reasoning and physical reality in robotics. I focus on
                    developing hierarchical systems that enable Vision-Language Models to generate probabilistic spatial
                    plans while ensuring they remain grounded in the kinematic and dynamic constraints of real robot
                    embodiments. My work addresses the fundamental challenge of long-horizon manipulation: how to
                    combine the semantic common sense of foundation models with the geometric rigor required for
                    reliable physical interaction.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:puru.ojha@research.iiit.ac.in">Email</a> &nbsp;/&nbsp;
                    <a href="Resume_Puru_Ojha.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="Research Statement 4 new.pdf">Research Statement</a> &nbsp;/&nbsp;
                    <a href="SOP_UMich.pdf">SOP</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/puru-ojha-a27632166/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/mechapuru">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/Profile_Pix.jpeg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/Profile_Pix.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    Modern robotics is fractured between two paradigms: end-to-end Vision-Language-Action models that
                    possess semantic understanding but often hallucinate physics, and classical planning systems that
                    are geometrically rigorous but rely on brittle, hand-engineered abstractions. My research aims to
                    bridge this divide through <strong>Hierarchical Action Reasoning Models (HARMs)</strong>—a framework
                    where high-level semantic reasoning is grounded via <strong>Probabilistic Spatial Contracts</strong>
                    and verified through <strong>Embodiment-Aware Causal Verification</strong>. Rather than treating
                    VLMs as isolated commanders that output deterministic actions, I develop systems where spatial
                    intent is represented as a distribution of feasible interactions, enabling robots to sample
                    trajectories that are both semantically logical and physically executable. This approach combines
                    the generalization of foundation models with the reliability required for real-world deployment.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src='images/placeholder.jpg' width="160">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a>
                    <span class="papertitle">ACMGVR: Architecturally Consistent Mazes for Games in Virtual
                      Reality</span>
                  </a>
                  <br>
                  <strong>Puru Ojha</strong>, Y. Raghu Reddy
                  <br>
                  <em>CHI PLAY</em>, 2024
                  <br>
                  <a href="https://doi.org/10.1145/3665463.3678818">Paper</a> | Code | Video | Poster
                  <p></p>
                  <p>
                    Engineered a framework for procedural generation of complex, structured environments for
                    benchmarking RL agents and studying sim-to-real transfer of navigational policies.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src='images/placeholder.jpg' width="160">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a>
                    <span class="papertitle">The POMS Effect: Measuring the Impact of Overlapping Architectures on User
                      Engagement in Virtual Reality</span>
                  </a>
                  <br>
                  <strong>Puru Ojha</strong>, Aaditya Narain, Y. Raghu Reddy
                  <br>
                  <em>ICVR</em>, 2025
                  <br>
                  <a href="Research Papers/The POMS Effect.pdf">Paper</a> | Code | Video | Poster
                  <p></p>
                  <p>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research Projects</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src='images/placeholder.jpg' width="160">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <span class="papertitle">Zero-Shot Policy Transfer for Cross-Embodiment Robotic Manipulation</span>
                  <br>
                  Paper | Code | Video | Poster
                  <br>
                  <p>Developed a zero-shot policy transfer framework for cross-embodiment manipulation, enabling a
                    generalist policy (π₀) trained on Franka Panda data to control a morphologically distinct uFactory
                    xArm without any fine-tuning. Implemented a Brownian bridge diffusion model for visual domain
                    adaptation, translating sensory inputs from the target robot (xArm) to the source domain (Panda) for
                    seamless, real-time policy execution. Engineered a robust policy transfer pipeline that leverages
                    real-world data, addressing generalization challenges across different robot hardware and
                    environments.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src='images/placeholder.jpg' width="160">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <span class="papertitle">Sequential Rearrangement Planning with Language-Guided Graph
                    Transformer</span>
                  <br>
                  Paper | Code | Video | Poster
                  <br>
                  <p>Designed a graph-based transformer model for predicting object removal sequences in cluttered
                    tabletop scenes, conditioned on natural language goals. Built a large-scale simulation and training
                    pipeline with expert A* supervision and PPO fine-tuning, achieving state-of-the-art performance over
                    GRN and NRP baselines.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src='images/placeholder.jpg' width="160">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <span class="papertitle">Enhanced Transformer-Based Framework for Grounded Image Situation
                    Recognition</span>
                  <br>
                  <a href="Research Papers/NeurIPS_2023.pdf">Paper</a> | Code | Video | Poster
                  <br>
                  <p>Developed a transformer framework for grounded situation recognition to enable robots to follow
                    complex natural language instructions. Improved noun grounding and verb prediction by integrating
                    CLIP and Faster-RCNN features into a CoFormer architecture, achieving state-of-the-art performance
                    on the SWiG dataset.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src='images/placeholder.jpg' width="160">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <span class="papertitle">Procedural Generation of Architecturally Consistent Simulation
                    Environment</span>
                  <br>
                  Paper | Code | Video | Poster
                  <br>
                  <p>Engineered a framework for procedural generation of complex, structured environments for
                    benchmarking RL agents and studying sim-to-real transfer of navigational policies. Designed a
                    graph-based algorithm to generate scalable, architecturally consistent layouts with dynamic
                    multi-path structures for realistic simulation.</p>
                </td>
              </tr>

            </tbody>
          </table>


          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
            <tbody>
              <tr>
                <td>
                  <h2>Miscellanea</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #fcb97d;">
                    <h2>Leadership & Mentorship</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <b>AI Olympiad Coach, IIIT-H:</b> Coached India’s IOAI 2025 team; taught RL and motion planning
                  bridging classical robotics with modern deep RL (DQN, PPO).
                  <br>
                  <b>Teaching Assistant, Mobile Robotics, IIIT-H:</b> Supported instruction and projects on robot
                  kinematics, SLAM, and path planning using ROS and Gazebo.
                </td>
              </tr>


              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #aaba9e;">
                    <h2>Technical Skills</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <b>Languages & Tools:</b> Python, C++, C#, Bash, Git, Linux, Tmux
                  <br>
                  <b>Robotics & Simulation:</b> ROS, PyBullet, Gazebo, MoveIt, RViz, OpenCV, PCL, Unity, Blender
                  <br>
                  <b>Deep Learning & RL:</b> PyTorch, Stable Baselines3, TensorFlow, CLIP, PointNet++, PyTorch
                  Geometric, RLlib, HDF5
                </td>
              </tr>

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's
                      website</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>